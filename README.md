# Cue â€” Real-time Augmented Memory for dementia patients

> **"Dementia takes away the context. We give you the Cue"**

Real-time Augmented Memory for dementia patients, powered by edge-AI face tracking (MediaPipe) and semantic vector search (Qdrant) to deliver instant, context-aware recognition.

## ğŸ¯ Features

### Patient Mode
- **Real-time Face Detection** â€” MediaPipe-powered face tracking
- **AR-style HUD** â€” Glassmorphic overlay showing name, relation, and routine activities
- **Memory Recording** â€” Audio recording with automatic transcription and summarization
- **Enhanced Audio Cues** â€” 4-sentence comfort whispers via ElevenLabs/Sarvam AI
- **Routine Extraction** â€” AI-detected patterns from conversations shown in HUD
- **Multi-Language Support** â€” English, Hindi, Tamil, Bengali, Telugu

### Caregiver Mode
- **Review Pending People** â€” See all unconfirmed faces detected
- **Confirm Identities** â€” Assign names and relationships
- **Manage Memories** â€” View and edit recorded memories
- **Language Selection** â€” Choose display/audio language for patient

## ğŸ—ï¸ Architecture

```
Frontend (React/Vite)          Backend (FastAPI)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Camera + MediaPipe  â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ InsightFace (ONNX)  â”‚
â”‚ HUD Overlay         â”‚â—€â”€â”€â”€â”€â”€â”€â”‚ Groq LLM            â”‚
â”‚ Audio Recording     â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ Groq Whisper        â”‚
â”‚ Caregiver Panel     â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚ Qdrant + Neo4j      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ ElevenLabs + Sarvam â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+
- Python 3.10+ (3.11 recommended)
- Qdrant Cloud account
- Neo4j Cloud account
- API Keys: Groq, ElevenLabs, Sarvam AI

---

### 1. Backend Setup

#### macOS / Linux

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Copy and configure environment
cp .env.example .env
# Edit .env with your API keys (see Environment Variables below)

# Run the server
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# (Optional) Run background worker for routine extraction
python -m app.workers.routine_worker
```

#### Windows

```powershell
cd backend

# Create virtual environment
python -m venv venv
venv\Scripts\activate

# Install dependencies (with CUDA support for GPU acceleration)
pip install -r requirements.txt

# For GPU acceleration (optional, requires CUDA 11.x):
pip uninstall onnxruntime
pip install onnxruntime-gpu

# Copy and configure environment
copy .env.example .env
# Edit .env with your API keys

# Run the server
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# (Optional) Run background worker for routine extraction
python -m app.workers.routine_worker
```

> **Note for Windows GPU Users:** InsightFace uses ONNX Runtime. Install `onnxruntime-gpu` for CUDA acceleration (5-10x faster face recognition).

---

### 2. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Run development server
npm run dev
```

---

### 3. Environment Variables

Create `backend/.env` with these keys:

```env
# Required
GROQ_API_KEY=your_groq_api_key
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your_qdrant_api_key
NEO4J_URI=neo4j+s://your-instance.databases.neo4j.io
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# TTS (Text-to-Speech)
ELEVENLABS_API_KEY=your_elevenlabs_key  # For English
SARVAM_API_KEY=your_sarvam_key          # For Hindi, Tamil, Bengali, Telugu
```

---

### 4. Access the App

- **Patient Mode:** http://localhost:5173
- **Caregiver Mode:** http://localhost:5173/caregiver
- **API Docs:** http://localhost:8000/docs

---

## ğŸ“¡ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/health` | GET | Health check |
| `/api/recognize-face` | POST | Recognize a face from image |
| `/api/hud-context?lang=` | POST | Get HUD content (with language) |
| `/api/whisper/{id}?lang=` | GET | Generate audio whisper cue |
| `/api/memory/save` | POST | Save memory from audio |
| `/api/caregiver/pending` | GET | Get pending people |
| `/api/caregiver/confirm` | POST | Confirm a person's identity |

## ğŸŒ Supported Languages

| Language | Code | TTS Provider | Voice |
|----------|------|--------------|-------|
| English | `en` | ElevenLabs | Jyot |
| Hindi | `hi` | Sarvam AI | Vidya |
| Tamil | `ta` | Sarvam AI | Vidya |
| Bengali | `bn` | Sarvam AI | Vidya |
| Telugu | `te` | Sarvam AI | Vidya |

## ğŸ” Key Principles

1. **No Identity Hallucination** â€” The LLM never guesses identities
2. **Caregiver Controls Truth** â€” Only caregivers can confirm identities
3. **TEMPORARY â†’ CONFIRMED** â€” New faces start as temporary until reviewed
4. **Privacy by Design** â€” No passive surveillance, explicit recording only
5. **Respectful Language** â€” Uses formal pronouns (à¤†à¤ª/à®¨à¯€à®™à¯à®•à®³à¯/à¦†à¦ªà¦¨à¦¿/à°®à±€à°°à±)

## ğŸ“ Project Structure

```
hackathon/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # Camera, HUD, LanguageSelector
â”‚   â”‚   â”œâ”€â”€ hooks/          # useFaceTracking, useAudioRecorder
â”‚   â”‚   â”œâ”€â”€ pages/          # PatientMode, CaregiverMode
â”‚   â”‚   â””â”€â”€ services/       # API client
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ routers/        # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/       # InsightFace, LLM, Sarvam, Whisper, DBs
â”‚   â”‚   â”œâ”€â”€ workers/        # Background routine worker
â”‚   â”‚   â”œâ”€â”€ data/           # Relations dictionary, templates
â”‚   â”‚   â””â”€â”€ models/         # Pydantic schemas
â”‚   â””â”€â”€ ...
â””â”€â”€ claude.md               # System design document
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Frontend | React + Vite |
| Face Tracking | MediaPipe (Browser) |
| Backend | FastAPI |
| Face Recognition | InsightFace (buffalo_s, ONNX) |
| LLM | Groq (llama-3.3-70b-versatile) |
| Speech-to-Text | Groq Whisper |
| Text-to-Speech | ElevenLabs (English) + Sarvam AI (Indian languages) |
| Translation | Sarvam AI (mayura:v1) |
| Vector DB | Qdrant Cloud |
| Graph DB | Neo4j Cloud |

## ğŸ–¥ï¸ Platform Support

| Platform | Status | Notes |
|----------|--------|-------|
| macOS (Apple Silicon) | âœ… Full | CoreML acceleration for ONNX |
| macOS (Intel) | âœ… Full | CPU-based ONNX |
| Windows (NVIDIA GPU) | âœ… Full | Install `onnxruntime-gpu` for CUDA |
| Windows (CPU) | âœ… Full | Slower face recognition |
| Linux | âœ… Full | GPU support with CUDA |

## ğŸ“„ License

MIT License â€” Built for hackathon demo purposes.
